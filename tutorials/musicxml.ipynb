{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0b631da",
   "metadata": {},
   "source": [
    "# MUSICXML SUPPORT IN MUSX\n",
    "\n",
    "An overview of loading and processing musical information stored in MusicXML files.\n",
    "<!-- An overview of loading and processing MusicXML files in musx and accessing/manipulating the loaded data. -->\n",
    "\n",
    "<hr style=\"height:1px;color:gray\">\n",
    "\n",
    "Notebook setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6409a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from musx import version, Note, pitch, Pitch, Interval\n",
    "from musx.mxml import notation\n",
    "from musx.mxml import musicxml\n",
    "print(f\"musx.version: {version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a9b1eb",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "MusicXML has become the defacto standard for encoding musical scores, and music projects now build databases of music information that can be imported and exported as MusicXML files. This notebook introduces the musx interface for loading and analyzing information encoded in MusicXml files.\n",
    "\n",
    "To start this overview, run the next cell to see the contents of the canonical 'Hello World' MusicXML file. \n",
    "The file's graphic rendition in MuseScore is displayed directly beneath it. Spend a few minutes looking at the image and trying to find its symbolic corollary in the MusicXML text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('support/HelloWorld.musicxml') as file:\n",
    "    print(file.read())\n",
    "    print(f\"file size: {file.tell()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c2b95",
   "metadata": {},
   "source": [
    "#### MuseScore image:\n",
    "<img src=\"support/HelloWorld.png\" width=\"250\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ab6fe",
   "metadata": {},
   "source": [
    "## The musx.mxml  module\n",
    "\n",
    "The musx.mxml module contains two layers of support for working with MusicXML:\n",
    "\n",
    "* The <b>musx.mxml.notation</b> module is a \"high-level\" interface that directly translates MusicXML entities into python objects representing western symbolic music notation. The high level also contains loading and mapping utilities to access music symbols and perform analysis.\n",
    "\n",
    "* The <b>musx.mxml.musicxml</b> module is a \"low-level\" interface containing Python class definitions for every entity defined in the MusicXML schema. The interface was generated from the official [MusicXml 4.0 partwise schema](https://www.w3.org/2021/06/musicxml40/musicxml-reference/elements/score-partwise/) using the [generateDS](https://pypi.org/project/generateDS/) Python package developed by Dave Kuhlman."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398dcf30",
   "metadata": {},
   "source": [
    "## The musx.mxml.notation interface\n",
    "\n",
    "The high-level notation interface translates a MusicXML score into a musx Notation object containing metadata and assorted musical objects such as parts, voices, bars, notes, rhythms, clefs, keys, etc.  Notations are designed for software programs to easily access score data and perform analysis; it currently has no graphical representation.  New notation symbols can be added to the module by associating the objects's MusicXML name with a parsing function and adding them to the module's parsing dictionary. For information about the existing notation symbols consult the mxml module documentation.\n",
    "\n",
    "The `notation.load()` function parses a MusicXML file and loads its symbols into a musx Notation object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08593bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = notation.load(\"support/HelloWorld.musicxml\")\n",
    "print(hello)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b7ca01",
   "metadata": {},
   "source": [
    "Use the `notation.print()` function to examine the musical information imported from the file. Start by printing metadata from the loaded notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8599394",
   "metadata": {},
   "outputs": [],
   "source": [
    "hello.print(metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1e1014",
   "metadata": {},
   "source": [
    "Call `print()` without arguments to display the objects contained in the Notation. Display indentation marks sub-object containers: notations contains parts; parts contain measures; measures contain elements (music symbols such as clefs, key signatures, and notes); and notes possess attributes:  metric start time, metric duration, pitch/rest/chord markers, amplitude, and (possibly) MusicXML markup such as staff and voice ids: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696f307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hello.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34a1f4",
   "metadata": {},
   "source": [
    "### Accessing notation objects\n",
    "\n",
    "Notation objects that contain other objects (e.g. Notation, Part, Measure) are traversable using Python's standard iteration constructs. As an example, this comprehension returns all the parts in the notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83da092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[part for part in hello]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4398e447",
   "metadata": {},
   "source": [
    "Here is how to return all the measures in all the parts of the notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc8880",
   "metadata": {},
   "outputs": [],
   "source": [
    "[measure for part in hello for measure in part]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db7a59",
   "metadata": {},
   "source": [
    "This returns all the entries in all the measures in all the parts of the notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf5ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[symbol for part in hello for measure in part for symbol in measure]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d5df1b",
   "metadata": {},
   "source": [
    "This returns only the notes in all the measures in all the ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[symbol for part in hello for measure in part for symbol in measure if isinstance(symbol, Note)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be0be7",
   "metadata": {},
   "source": [
    "It is also possible to access sub-container data using indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7695fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hello.parts[0].measures[0].elements[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a78d43",
   "metadata": {},
   "source": [
    "### Timepoints\n",
    "\n",
    "The previous examples demonstrate how to access notation data in serial order. However, music consist of parts performed in *parallel*, such that the sound at any given time is the sum of all the sounding notes in all parts at that time.  In order to access 'vertical' sonorities in serial (timewise) order, musx provides an analytical structure called a `Timepoint` that containing the set of all vertical notes that are sounding when any note begins. Each Timepoint contains an `onset` (beat) in a measure and a `notemap`: a dictionary whose keys are *part.voice* identifiers and whose values are the notes or chords that begin at that Timepoint's beat.\n",
    "\n",
    "`Notation.timepoints(trace=False, spanners=False, flatten=False)`\n",
    "\n",
    "Timepoints() returns a list of all the timepoints in the score.  This list is normally organized into sublist measures; set flatten to true to collect a flat list of all the timepoints in the score. Set the trace parameter to True if you want timepoints printed to the terminal as they are collected. The spanners parameter is more complicated. If spanners is false then a timepoint collects only notes that *begin* at the current timepoint.  If spanners is set to true, then notes that began earlier than the current timepoint but are still sounding during the timepoint are added to the timepoint and marked as a *spanner*. A spanner is distinguishable from other notes in the timepoint by virtue of its earlier start time than the timepoint and its inclusion in the Timepoint.spanners list. Spanners will appear in the trace surrounded by repeat signs '::'.\n",
    "\n",
    "Here is an example of a second species counterpoint. We will first convert the score into timepoints without spanners and then determine the \"horizontal intervals\" between melodic notes in each part as well as the \"vertical intervals\" between the notes of the two parts:\n",
    "\n",
    "<img src=\"support/2-000-A_sz18.png\" width=\"600\" />\n",
    "\n",
    "<!-- \n",
    "<img src=\"support/Aus_meines_Hertzens_Grunde.png\" width=\"500\" />\n",
    "\n",
    "bach = notation.load(\"support/Aus_meines_Herzens_Grunde.musicxml\")\n",
    "bach.timepoints()\n",
    "chopin = notation.load(\"support/chopin_prelude_op28_no20.xml\")\n",
    "chopin.timepoints(spanners=True)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c2bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = notation.load(\"support/species2.musicxml\")\n",
    "print(species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbcc99",
   "metadata": {},
   "source": [
    "Parse the score into timepoints organized into measures (sublists) <!-- All the measures but the last contain two timepoints, the first timepoint starts at beat 0 and the second at 1/2 (i.e. half note): -->:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b0bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline = species.timepoints()\n",
    "timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87beeae1",
   "metadata": {},
   "source": [
    "The timeline output from the previous cell contains 10 measures (sublists), each measure but the last has two timepoints. The first measure contains two timepoints:\n",
    "\n",
    "    [<Timepoint: 0     P1.1: (C4, 1/2), P2.1: (C3, 1)>,\n",
    "     <Timepoint: 1/2   P1.1: (B3, 1/2)>]\n",
    "    \n",
    "The first timepoint starts at time 0 and contains two notes: the C4 in the upper voice (part 1, voice 1) and the C3 in the lower voice (part 2, voice 1). The second timepoint starts at time 1/2 and contains only the upper note (part 1, voice 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6685564",
   "metadata": {},
   "source": [
    "#### Analysis 1: determine the melodic intervals in each part\n",
    "\n",
    "In order to determine the melodic in each part we need to know which notes belong to which part and voice. This can be determined by looking at the *part.voice* identifiers in the timepoint's *notemap* dictionary: notes for the top melody are identified as P1.1 (part 1 voice 1), and notes for the lower melody are P2.1 (part 2 voice 1).\n",
    "\n",
    "This can been seen very clearly by looking at the actual attribute data in the first timepoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0ab954",
   "metadata": {},
   "outputs": [],
   "source": [
    "for timepoint in timeline[0]: print(f\"onset: {timepoint.onset} notemap: {timepoint.notemap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d1edd",
   "metadata": {},
   "source": [
    "Since the identifiers \"P1.1\" and \"P2.1\" distinguish the melodic lines they can be used to collect the notes for each part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4e09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topmelody, bottommelody = [], []\n",
    "for measure in timeline:\n",
    "    for timepoint in measure:\n",
    "        for identifier in timepoint.notemap:\n",
    "            pitch = timepoint.notemap[identifier].pitch\n",
    "            if identifier == \"P1.1\":\n",
    "                topmelody.append(pitch)\n",
    "            else:\n",
    "                bottommelody.append(pitch)\n",
    "\n",
    "print(f\"Top melody:\\n{[n.string() for n in topmelody]}\", end=\"\\n\\n\")\n",
    "print(f\"Bottom melody:\\n{[n.string() for n in bottommelody]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e033594",
   "metadata": {},
   "source": [
    "To determine the melodic intervals between notes in a melody, step pairwise through the melody and collect the interval between each pair of notes. Descending melodic intervals will appear with a minus sign. After executing the next cell compare its results with the output from the previous cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4731c7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topintervals = [Interval(left, right) for left, right in zip(topmelody, topmelody[1:])]\n",
    "bottomintervals = [Interval(left, right) for left, right in zip(bottommelody, bottommelody[1:])]\n",
    "\n",
    "print(f\"Top melody intervals:\\n{[i.string() for i in topintervals]}\", end=\"\\n\\n\")\n",
    "print(f\"Bottom melody intervals:\\n{[i.string() for i in bottomintervals]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a8aee",
   "metadata": {},
   "source": [
    "#### Analysis 2: Determine the vertical intervals between parts\n",
    "\n",
    "Determining the vertical intervals between parts is more challenging because the parts do not have the same number of notes: the first timepoint in each measure has a top and bottom note but the second timepoint has only the top note:\n",
    "\n",
    "                    Top        Bottom                   \n",
    "    <Timepoint: 0   (C4, 1/2), (C3, 1)> \n",
    "                    Top        (no bottom note!)\n",
    "    <Timepoint: 1/2 (B3, 1/2)>\n",
    "    \n",
    "\n",
    "This is a case where spanning is useful: by including spanners, each timepoint will included all the actively sounding notes regardless of part or voice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5d4191",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline = species.timepoints(spanners=True)\n",
    "timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f198e0",
   "metadata": {},
   "source": [
    "By parsing the timeline with spanners=True, the second timepoint in each measure now includes two notes: the note in the top voice that starts on the second beat, as well as the whole note that is still sounding from the downbeat in the bottom part. (Spanning notes are marked by !! delimiters in the output.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3268021",
   "metadata": {},
   "outputs": [],
   "source": [
    "verticalintervals = []\n",
    "for measure in timeline:\n",
    "    for timepoint in measure:\n",
    "        top, bottom = timepoint.notemap.values()\n",
    "        verticalintervals.append(Interval(bottom.pitch,top.pitch))\n",
    "        \n",
    "print(f'Vertical intervals:\\n{[i.string() for i in verticalintervals]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6cbf14",
   "metadata": {},
   "source": [
    "## The lowlevel musicxml interface\n",
    "\n",
    "To work with the low-level interface you must be familiar with MusicXML entities a well as the Python classes in the musx.mxml.musicxml.py module.\n",
    "\n",
    "`musicxml.parse(inFileName, silence=False, print_warnings=True)`\n",
    "\n",
    "The `musicxml.parse()` function transforms a MusicXML text file into a graph of Python XML objects. The return value is the root node of the graph.  Note: the first time `parse()` is called it may take Python some seconds to load the very large MusicXML class definition file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e0fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = musicxml.parse(\"support/HelloWorld.musicxml\", silence=True)\n",
    "print(f\"root node: {root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108e30e",
   "metadata": {},
   "source": [
    "Define a helper function that only returns attributes defined in the MusicXML schema and omitting any attributes that are inherited from the generateDS implementation itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a3f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schema_attrs(node):\n",
    "    return [d for d in dir(node) if not (callable(getattr(node, d)) \n",
    "                                         or d.startswith(('_', 'gds', 'subclass', 'superclass', 'tzoff'))\n",
    "                                         or d.endswith('_'))]\n",
    "\n",
    "print(f\"schema_attrs: {schema_attrs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d02d112",
   "metadata": {},
   "source": [
    "Display the MusicXml attributes of the root node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01d6a5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "schema_attrs(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45cf1f9",
   "metadata": {},
   "source": [
    "The root.part attribute will contain all the part objects in the score (this score has only one part definition):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b6d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root.part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f391b97e",
   "metadata": {},
   "source": [
    "Show the MusicXML attributes of a part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7437c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_attrs(root.part[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdd1954",
   "metadata": {},
   "source": [
    "Every MusicXML part will have a unique id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd8852",
   "metadata": {},
   "outputs": [],
   "source": [
    "root.part[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3944c47c",
   "metadata": {},
   "source": [
    "A part can contain one or more measures; our score has only one measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29980597",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root.part[0].measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c9116",
   "metadata": {},
   "source": [
    "Display the attributes of a measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a0643",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_attrs(root.part[0].measure[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a45c3ce",
   "metadata": {},
   "source": [
    "The note attribute of a measure holds the temporal events contained in the measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a41bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "root.part[0].measure[0].note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afce360e",
   "metadata": {},
   "source": [
    "Display the attributes of a note:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd25b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(schema_attrs(root.part[0].measure[0].note[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac3f27",
   "metadata": {},
   "source": [
    "This example displays the staff, pitch, rest, and duration values for each of the notes in measure 0. Notice that the first note (the note at index 0) has a pitch and does not have a rest while the second is reversed, it has a rest and not a pitch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a2172",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Note 0 (top staff):\")\n",
    "print(f\"  staff: {root.part[0].measure[0].note[0].staff}\")\n",
    "print(f\"  pitch: {root.part[0].measure[0].note[0].pitch}\")\n",
    "print(f\"  rest: {root.part[0].measure[0].note[0].rest}\")\n",
    "print(f\"  duration: {root.part[0].measure[0].note[0].duration}\")\n",
    "print(\"Note 1 (bottom staff):\")\n",
    "print(f\"  staff: {root.part[0].measure[0].note[1].staff}\")\n",
    "print(f\"  pitch: {root.part[0].measure[0].note[1].pitch}\")\n",
    "print(f\"  rest: {root.part[0].measure[0].note[1].rest}\")\n",
    "print(f\"  duration: {root.part[0].measure[0].note[1].duration}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec49f23",
   "metadata": {},
   "source": [
    "The first note (the note at index 0) contains a pitch, here are its attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9cd179",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_attrs(root.part[0].measure[0].note[0].pitch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f038ba4a",
   "metadata": {},
   "source": [
    "The step of a pitch is a string letter and the octave of a pitch is an int:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d33c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = root.part[0].measure[0].note[0].pitch.step\n",
    "octave = root.part[0].measure[0].note[0].pitch.octave\n",
    "info = [step, octave]\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eeefd3",
   "metadata": {},
   "source": [
    "Given the step and octave values of a MusicXML pitch it is easy to convert it to a musx Pitch and access the information in different ways. Representations for any other MusicXml attributes can be developed in similar ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88ded8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = Pitch(\"\".join([str(i) for i in info]))\n",
    "print(f\"Pitch:  {repr(p)}\")\n",
    "print(f\"keynum: {p.keynum()}\")\n",
    "print(f\"hertz:  {p.hertz()}\")\n",
    "print(f\"pc:     {p.pc()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
